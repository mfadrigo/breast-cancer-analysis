# -*- coding: utf-8 -*-
"""MLProjectBreastCancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FeFnprhhGko8abYd3BZJhmZTBiCcKSt5
"""

import numpy as np
import pandas as pd

from google.colab import files
uploaded = files.upload()       # Will prompt you to select a file

import io
data = pd.read_csv(io.BytesIO(uploaded['data.csv'])) # Dataset is now stored in a Pandas Dataframe

"""# Exploratory Data Analysis"""

print("There are", data.shape[0], "rows and", data.shape[1], "columns in the dataset.")

data.info()

"""The last column in the dataset contains all NA values, and so we remove it.

## Cleaning Data
"""

data = data.drop(['Unnamed: 32'], axis = 1)
print("After data cleaning, there are", data.shape[0], "observations and", data.shape[1], "features in the dataset.")

data.head()

data.isna().any()

"""There are no features that contains NA values.

## Numerical Features: Descriptive Statistics
"""

data.describe()

"""## Categorical Features: Descriptive Statistics"""

data.describe(include = 'O')

"""## Feature Design

Getting features that have moderate to high correlation with Diagnosis. Then assigning it to new dataframe.
"""



"""## Data Visualization

Selecting the mean features, since the other features are related.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express
import seaborn
seaborn.set()

sns.countplot(data,x="diagnosis")

plt.xlabel("Breast Cancer Diagnosis")
plt.ylabel("Count of Benign/Malignant Breast Cancer")
plt.title("Total Breast Cancer Diagnosis Counts")

plt.show()

"""## Feature Scaling"""

X = data.iloc[:, 2:12]
X.hist(figsize=(20, 15));

"""Most of the distributions are right-skewed, which indicates that not all features follow a standard normal distribution (mean=0, variance=1), which would present itself as a bell-shaped curve. Their scales also vary, which may cause features with higher scales/magnitudes  contributing more to the model training process. However, we want to ensure that all features are contributing equally to the model training process.

Feature scaling refers to the process of transforming the values of the features to a similar scale. This process is necessary for our SVM model, which is distance-based, so that features with high magnitudes do not dominate the distance calculations. 
"""

from sklearn_pandas import DataFrameMapper
from sklearn.preprocessing import StandardScaler

mean_data = data.iloc[:, 2:12] # Each observation has 10 means corresponding to 10 main features (i.e. radius, texture, smoothness, etc.)

# STANDARDIZING MEAN
mapper = DataFrameMapper([(mean_data.columns, StandardScaler())])
scaled_features = mapper.fit_transform(mean_data.copy(), 10)
scaled_features_df = pd.DataFrame(scaled_features, index=mean_data.index, columns=mean_data.columns)

print(scaled_features_df.head(5))

scaled_features_df.hist(figsize=(20, 15));

scaled_features_df  = pd.concat([data.iloc[:,0:2], scaled_features_df.iloc[:,:]], axis=1) # concatenate id, diagnosis with standardized means
scaled_features_df

# Convert Standardized Data Frame from Wide Format to Long Format
data_melt = pd.melt(scaled_features_df, id_vars = ["id", "diagnosis"], var_name = "features", value_name = "std_mean")
data_melt.head(5)


# Boxplot for Each Main Feature Mean (i.e. radius_mean, texture_mean, smoothness_mean, etc.)
g = sns.catplot(data= data_melt, x = 'diagnosis', y = 'std_mean', col = 'features', kind = 'box', col_wrap = 2)

"""After feature scaling using standardization, each boxplot of the mean features are scaled to have zero mean and unit variance, mitigating the effects of features with extreme values, which can affect the model's performance."""

temp = pd.concat([data.iloc[:,1], mean_data.iloc[:,:]], axis=1)
sns.pairplot(temp, hue="diagnosis")
plt.plot()

"""From the pairplot, strong linear positive correlations are seen between the features, such as perimeter mean and radius mean, suggesting that the model can be affected negatively. There may be overfitting, unstable or inaccurate model coefficients, and become more difficult to interpret feature importance. 

To counter this, SVM will include principle component analysis, which can mitigate issues by utilizing dimensionality reduction, improving our performance accuracy and generalization when comparing our model to the test data. Since the data is relatively small, PCA is not necessarily needed, but is good practice. 
"""

plt.figure(figsize=(12, 9))

plt.title("Correlation Graph")
cols = list(mean_data.columns)
# print(cols)

sns.heatmap(data[cols].corr(), annot=True);

"""## Feature Design

Transforming categorical diagnosis feature into 0/1 to prepare for modeling.
"""

from sklearn import preprocessing
label_encoder= preprocessing.LabelEncoder()
data['diagnosis'] = label_encoder.fit_transform(data['diagnosis']) 
data['diagnosis'].head()

# Reassign Numerical Labels to Diagnosis Column
data['diagnosis']= label_encoder.fit_transform(data['diagnosis'])

data['diagnosis'].value_counts()

"""## Model Exploration"""

from sklearn.model_selection import train_test_split

# Split the data to train and test the model (.8,.2) --> 80% assigned to train, 20% assigned to validate/test
mean_features = data.iloc[:, 1:12]

X = mean_features.drop(['diagnosis'], axis=1)
Y = mean_features['diagnosis']

Xtr, Xva, Ytr, Yva = train_test_split(X, Y, test_size = 0.2, random_state=10)

"""## SVM

Note, code with all outputs in personal "copy"
"""

from sklearn import svm
from sklearn import metrics
from sklearn.preprocessing import StandardScaler

"""####Feature Scaling for SVM"""

# Perform feature scaling on the training data . Train the machine learning model on the scaled training data.
scaler = StandardScaler()  
Xtr =  scaler.fit_transform(Xtr)

# Use the same scaling transformation to transform the testing data.
Xva = scaler.transform(Xva)
# Evaluate the performance of the model on the scaled testing data.

"""####Model

Our model will include a soft margin classifier, since we want more robustness in regards to outliers and handle possible non-linear separable data.

Here, choosing linear kernel SVM for linearly separable data first, seeing that the training set is very large/has plenty of features. We will utilize the hinge loss function with the linear kernel svm for regularization.
"""

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC

from sklearn.svm import LinearSVC

cancer = LinearSVC(loss='hinge', dual=True)

cancer = svm.SVC(kernel='linear', C = 0.05) # Linear Kernel w/ parameter
#Train model
cancer.fit(Xtr, Ytr)

#Predict response 
X_pred_train = cancer.predict(Xtr)
X_pred_test = cancer.predict(Xva)

# Model Accuracy
train_score = metrics.accuracy_score(Ytr, X_pred_train)*100
test_score = metrics.accuracy_score(Yva, X_pred_test)*100

print(cancer)
print("Train Accuracy:",round(train_score,2),"%")
print("Test Accuracy:",round(test_score,2),"%")

"""#### Hypertuning and Cross Validation

Since random hyperparameters were utilized, we would like to optimize them to improve model performance and accuracy. For imbalanced data, we would utilize a class-weighted SVM, gives different weights to both the majority and minority classes and the classification of the classes will be influenced during the training phase. This would penalize the misclassification made by the minority class by setting a higher class weight and at the same time reducing weight for the majority class.
"""

kernels = ['linear', 'poly', 'rbf']
C_values = [0.001, 0.01, 0.1, 1, 10, 100]
gamma_values = [0.001, 0.01, 0.1, 1, 10, 100]

best_model = None
best_kernel = None
best_score = 0

for kernel in kernels:
  for C in C_values:
        for gamma in gamma_values:
            svc = svm.SVC(kernel=kernel, class_weight='balanced', C=C, gamma=gamma,probability=True)
            svc.fit(Xtr, Ytr)
            y_pred_val = svc.predict(Xva)
            score = metrics.accuracy_score(Yva, y_pred_val)*100
            if score > best_score:
                best_model = svc
                best_kernel = kernel
                best_score = score

# Evaluate the best model on the test set
y_pred_test = best_model.predict(Xva)
test_score = metrics.accuracy_score(Yva, y_pred_test)*100

print("Best model with hyperparameters:")
print(best_model)
print("Kernel:", best_kernel)
print("Validation accuracy:", round(best_score, 2), "%")
print("Test accuracy:", round(test_score, 2), "%")


# Evaluate the best model on the test set
y_pred_test = best_model.predict(Xva)
test_score = metrics.accuracy_score(Yva, y_pred_test)*100

print("Best model with hyperparameters:")
print(best_model)
print("Kernel:", best_kernel)
# print("Validation accuracy:", round(best_score, 2), "%")
# print("Test accuracy:", round(test_score, 2), "%")

svm_best = best_model
svm_best.fit(Xtr, Ytr)

y_pred_svm = svm_best.predict(Xva)

svm_cm = confusion_matrix(Yva,y_pred_svm)
print("Confusion Matrix generated from best svm model with optimal hyperparameters:")
print(confusion_matrix(Yva,y_pred_svm),"\n")
print("Classification Report")
print(classification_report(Yva,X_pred_test))

# Recall: Model's ability to correctly detect positive (malignant) cases. --> Out of the positive class, how many did I predict were positive?
## num true positive / (num true positive) + (num false negative)
recall = recall_score(Yva, y_pred_svm)
print("\n Recall Score on test data, using best svm model with optimal hyperparameters: {:.2f}".format(recall))
print("Out of all the malignant tumors in the dataset, {:.2f}% of them were classified as malignant by the model.".format(recall*100))

# Precision: Accuracy of the model's positive predictions --> Out of the positive predictions, how many were actually positive?
## num true positive / (num true positive) + (num false positive)
precision = precision_score(Yva, y_pred_svm)
print("\n Precision Score on test data, using best svm model with optimal hyperparameters: {:.2f}".format(precision))
print("Out of the tumors that the model classified as malignant, {:.2f}% of them were actually malignant.".format(precision*100))

# Accuracy: Proportion of Correctly Predicted Cases (Class 0 & Class 1) = (num true positive + num true negative) / total number of predictions
accuracy = svm_best.score(Xva, Yva)
print("\n Accuracy on test data, using best svm model with optimal hyperparameters: {:.2f}".format(accuracy))
print("Out of all the tumors in the dataset, {:.2f}% of them were actually correctly classified by the model.".format(accuracy*100))

import time
kernels = ['linear', 'poly', 'rbf']
C_values = [0.001, 0.01, 0.1, 1, 10, 100]
gamma_values = [0.001, 0.01, 0.1, 1, 10, 100]

best_model = None
best_kernel = None
best_score = 0

starttime= time.process_time()

for kernel in kernels:
  for C in C_values:
        for gamma in gamma_values:
            svc = svm.SVC(kernel=kernel, class_weight='balanced', C=C, gamma=gamma,probability=True)
            svc.fit(Xtr, Ytr)
            y_pred_val = svc.predict(Xva)
            score = metrics.accuracy_score(Yva, y_pred_val)*100
            if score > best_score:
                best_model = svc
                best_kernel = kernel
                best_score = score     #got the best hyperparameters/model from training and then also tested with test data

# Evaluate the best model on the test set (overall)
y_pred_test = best_model.predict(Xva) #insert X test set in our best model to get y_pred
test_score = metrics.accuracy_score(Yva, y_pred_test)*100

print("Best parameters:",best_model)
print("Kernel:", best_kernel)
# print("Validation accuracy:", round(best_score, 2), "%")
# print("Test accuracy:", round(test_score, 2), "%")


# Evaluate the best model on the test set
y_pred_test = best_model.predict(Xva)
test_score = metrics.accuracy_score(Yva, y_pred_test)*100

svm_best = best_model
svm_best.fit(Xtr, Ytr)

y_pred_svm = svm_best.predict(Xva)

endtime= time.process_time()

totaltime= endtime- starttime
print(f"Total time taken by 5-fold Cross Validation with SVM is" ,(totaltime),'seconds')
print("Average Perfomance Measure of SVM Model using General Cross Validation")
# svm_cm = confusion_matrix(Yva,y_pred_svm)
# print("Confusion Matrix generated from best svm model with optimal hyperparameters:")
# print(confusion_matrix(Yva,y_pred_svm),"\n")
print("Classification Report")
print(classification_report(Yva,y_pred_svm))

# Recall: Model's ability to correctly detect positive (malignant) cases. --> Out of the positive class, how many did I predict were positive?
## num true positive / (num true positive) + (num false negative)
recall = recall_score(Yva, y_pred_svm)
print("Recall Score: {:.2f}".format(recall))
# print("Out of all the malignant tumors in the dataset, {:.2f}% of them were classified as malignant by the model.".format(recall*100))

# Precision: Accuracy of the model's positive predictions --> Out of the positive predictions, how many were actually positive?
## num true positive / (num true positive) + (num false positive)
precision = precision_score(Yva, y_pred_svm)
print("Precision Score: {:.2f}".format(precision))
# print("Out of the tumors that the model classified as malignant, {:.2f}% of them were actually malignant.".format(precision*100))

# Accuracy: Proportion of Correctly Predicted Cases (Class 0 & Class 1) = (num true positive + num true negative) / total number of predictions
accuracy = svm_best.score(Xva, Yva)
print("Accuracy: {:.2f}".format(accuracy))
# print("Out of all the tumors in the dataset, {:.2f}% of them were actually correctly classified by the model.".format(accuracy*100))

#Classification Report (Training) To see Overfitting (If F1 values in train/test are high, not overfitting)
svm_best.fit(Xtr,Ytr)
y_train_pred = svm_best.predict(Xtr)
print("Classification Report (Train)")
print(classification_report(Ytr,y_train_pred))

"""##### Bayesian Optimization

From the best SVM model with the optimized hyperparameters, we could also conduct another method to test the generalization of the model to new test data. Since random hyperparameters and kernels were chosen, will use Bayesian optimization based off of the hypertuned model above with the rbf kernel.
Other methods of cross-validation such as GridSearch and RandomSearch were too costly in time complexity, due to the moderately sized data, and specific hyperparameters desired. Bayesian Optimization is suited for datasets with high dimensionalities.
"""

# %pip install bayesian-optimization==1.4.1
from bayes_opt import BayesianOptimization
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score


# Define the SVM model to be optimized
def svm_cv(C, gamma):
    svm = SVC(C=C, gamma=gamma, kernel='rbf',probability=True)
    scores = cross_val_score(svm, X, Y, cv=5, scoring='accuracy')
    return scores.mean()

# Define the search space for the hyperparameters
param_bounds = {'C': (0.1, 100), 'gamma': (0.01, 10)}

# Define the Bayesian Optimization object and run the optimization
svm_bayesopt = BayesianOptimization(svm_cv, param_bounds,verbose=0)
svm_bayesopt.maximize(init_points=5, n_iter=20)

# # Print the best hyperparameters found by Bayesian Optimization
svm_bayesopt.maximize(init_points=5, n_iter=20)

# Print the best hyperparameters found by Bayesian Optimization
best_params = svm_bayesopt.max['params']
print(f"Best parameters: {best_params}")

svm_best = SVC(**best_params,probability=True)
svm_best.fit(Xtr, Ytr)

y_pred_svm = svm_best.predict(Xva)

svm_cm = confusion_matrix(Yva,y_pred_svm)
print("Confusion Matrix generated from best svm model with optimal hyperparameters:")
print(confusion_matrix(Yva,y_pred_svm),"\n")
print("Classification Report")
print(classification_report(Yva,X_pred_test))

# Recall: Model's ability to correctly detect positive (malignant) cases. --> Out of the positive class, how many did I predict were positive?
## num true positive / (num true positive) + (num false negative)
recall = recall_score(Yva, y_pred_svm)
print("\n Recall Score on test data, using best svm model with optimal hyperparameters: {:.2f}".format(recall))
print("Out of all the malignant tumors in the dataset, {:.2f}% of them were classified as malignant by the model.".format(recall*100))

# Precision: Accuracy of the model's positive predictions --> Out of the positive predictions, how many were actually positive?
## num true positive / (num true positive) + (num false positive)
precision = precision_score(Yva, y_pred_svm)
print("\n Precision Score on test data, using best svm model with optimal hyperparameters: {:.2f}".format(precision))
print("Out of the tumors that the model classified as malignant, {:.2f}% of them were actually malignant.".format(precision*100))

# Accuracy: Proportion of Correctly Predicted Cases (Class 0 & Class 1) = (num true positive + num true negative) / total number of predictions
accuracy = svm_best.score(Xva, Yva)
print("\n Accuracy on test data, using best svm model with optimal hyperparameters: {:.2f}".format(accuracy))
print("Out of all the tumors in the dataset, {:.2f}% of them were actually correctly classified by the model.".format(accuracy*100))

# %pip install bayesian-optimization==1.4.1
from bayes_opt import BayesianOptimization
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

starttime= time.process_time()

# Define the SVM model to be optimized
def svm_cv(C, gamma):
    svm = SVC(C=C, gamma=gamma, kernel='rbf',probability=True)
    scores = cross_val_score(svm, X, Y, cv=5, scoring='accuracy')
    return scores.mean()

# Define the search space for the hyperparameters
param_bounds = {'C': (0.1, 100), 'gamma': (0.01, 10)}

# Define the Bayesian Optimization object and run the optimization
svm_bayesopt = BayesianOptimization(svm_cv, param_bounds,verbose=0)
svm_bayesopt.maximize(init_points=5, n_iter=20)

# # Print the best hyperparameters found by Bayesian Optimization
svm_bayesopt.maximize(init_points=5, n_iter=20)

# Print the best hyperparameters found by Bayesian Optimization
best_params = svm_bayesopt.max['params']
print(f"Best parameters: {best_params}")

svm_bestB = SVC(**best_params,probability=True)
svm_bestB.fit(Xtr, Ytr)

y_pred_svm = svm_bestB.predict(Xva)

endtime= time.process_time()

totaltime= endtime- starttime
print(f"Total time taken by 5-fold Cross Validation with SVM is" ,(totaltime),'seconds')

print("Average Perfomance Measure of SVM Model using Bayesian Optimization")

print("Classification Report")
print(classification_report(Yva,y_pred_svm))

#On Test Data
# Recall: Model's ability to correctly detect positive (malignant) cases. --> Out of the positive class, how many did I predict were positive?
## num true positive / (num true positive) + (num false negative)
recall = recall_score(Yva, y_pred_svm)
print("Recall Score: {:.2f}".format(recall))
# print("Out of all the malignant tumors in the dataset, {:.2f}% of them were classified as malignant by the model.".format(recall*100))

# Precision: Accuracy of the model's positive predictions --> Out of the positive predictions, how many were actually positive?
## num true positive / (num true positive) + (num false positive)
precision = precision_score(Yva, y_pred_svm)
print("Precision Score: {:.2f}".format(precision))
# print("Out of the tumors that the model classified as malignant, {:.2f}% of them were actually malignant.".format(precision*100))

# Accuracy: Proportion of Correctly Predicted Cases (Class 0 & Class 1) = (num true positive + num true negative) / total number of predictions
accuracy = svm_best.score(Xva, Yva)
print("Accuracy: {:.2f}".format(accuracy))
# print("Out of all the tumors in the dataset, {:.2f}% of them were actually correctly classified by the model.".format(accuracy*100))

#Classification Report (Training) To see Overfitting
svm_bestB.fit(Xtr,Ytr)
y_train_pred_2 = svm_bestB.predict(Xtr)
print("Classification Report (Train)")
print(classification_report(Ytr,y_train_pred_2))

"""#### Classifier Calibration

Although we are able to detect the accuracy of the model overall with the test set, we would also like to look at the estimated class probabilities, the confidence of the diagnosis of benign or malignant.
This is especially important since having a strong prediction/certainty of the diagnosis will greatly impact the patient. We would like a strong positive correlation between the computed probability and proportion of positive "malign" tumor.

We will calibrate the model using the Platt's method,which is suitable for smaller data and recheck accuracy, and visualize with an ROC curve. An ROC curve However, there may be a tradeoff between accuracy and calibration.
"""

#First Best model
from sklearn.metrics import roc_curve, auc

y_pred_svm_cal = svm_best.predict_proba(Xva)[:, 1] # fix typo here
svm_cm = confusion_matrix(Yva, best_model.predict(Xva)) # use best_model here

# Calculate the False Positive Rate (FPR) and True Positive Rate (TPR) for different threshold values
fpr, tpr, thresholds = roc_curve(Yva, y_pred_svm_cal) # use y_pred_svm_cal here

# Plot the ROC curve
import matplotlib.pyplot as plt

plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('First Best Model ROC Curve')

# Calculate the AUC score
auc_score = auc(fpr, tpr)
print('AUC: %.3f' % auc_score)

# Calculate the False Positive Rate (FPR) and True Positive Rate (TPR) for different threshold values
fpr_2, tpr_2, thresholds_2 = roc_curve(Yva, y_pred_svm_cal_2) # use y_pred_svm_cal here

# Plot the ROC curve
import matplotlib.pyplot as plt

plt.plot(fpr_2, tpr_2)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Second Bayesian Method Best Model ROC Curve')

# Calculate the AUC score
auc_score = auc(fpr_2, tpr_2)
print('AUC: %.3f' % auc_score)

"""### Random Forest

The dataset is imbalanced, in which 63% of the observations belong to class 0 (benign), or the majority class, and 37% of the observations belong to class 1 (malignant), or the minority class. As a result, a random forest model will most likely select features that are better at predicting the majority class, during the tree construction process. While the model may accurately predict instances in the majority class, it will perform poorly in predicting instances in the minority class. 


To address this issue, we will use Stratified K-fold cross validation, and GridSearchCV combined. Stratified K-fold cross validation  is essentially K-fold cross validation, except that it ensures that within each fold, the proportion of each class is roughly equal. We will also use GridSearchCV for hyperparameter tuning which will search over a range of hyperparameters to find the combination that yields the best performance on the validation set. Using these two methods together will allow us to make accurate hyperparameter choices.  

Random Forest is also an ensemble learning algorithm that works by combining multiple decision trees, each trained on a random subset of features. Random Forest is known to be more robust to correlated features because it selects different subsets of features at each node of each tree, which reduces the risk of overfitting and improves the model's generalization ability. Additionally, the averaging process used in Random Forest helps to further reduce the effect of correlated features, as each tree in the forest has a chance to correct the errors made by the other trees.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV

# Split the data to train and test the model (.8,.2) --> 80% assigned to train, 20% assigned to validate/test
mean_features = data.iloc[:, 1:12]

X = mean_features.drop(['diagnosis'], axis=1)
Y = mean_features['diagnosis']

Xtr, Xva, Ytr, Yva = train_test_split(X, Y, test_size = 0.2, random_state=10)

num_folds = 10
skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

# Define the hyperparameters to tune
param_grid = {'n_estimators': [50, 100, 200], # number of decision trees in forest
              'max_features': range(1,11), # maximum number of features to consider when splitting a node in each decision tree in the random forest.
              'max_depth': range(2, 15), # maximum depth of the tree
              'min_samples_leaf': range(1,3), # minimum number of samples to include in a leaf node
              'criterion':['gini','entropy']}

rf = RandomForestClassifier(random_state=42, n_jobs=-1)

gcv = GridSearchCV(rf, param_grid, n_jobs=-1, cv=skf, scoring='recall')

gcv.fit(Xtr, Ytr)

from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import time

best_params = gcv.best_params_
print(best_params)
best_score = gcv.best_score_

starttime= time.process_time()
# Fit the best random forest model obtained from GridSearchCV
best_rf = RandomForestClassifier(random_state=42, n_jobs=-1, **best_params)
best_rf.fit(Xtr, Ytr)

# predictions on test data generated from best random forest model
y_pred = best_rf.predict(Xva)

endtime= time.process_time()

totaltime= endtime- starttime
print(f"Total time taken by Random Forest Classifier is" ,(totaltime),'seconds')

# Confusion Matrix
cm = confusion_matrix(Yva, y_pred)
print("Confusion Matrix generated from best random forest model with optimal hyperparameters: \n", cm)

# Recall: Model's ability to correctly detect positive (malignant) cases. --> Out of the positive class, how many did I predict were positive?
## num true positive / (num true positive) + (num false negative)
recall = recall_score(Yva, y_pred)
print("\n Recall Score on test data, using best random forest model with optimal hyperparameters: {:.2f}".format(recall))
print("Out of all the malignant tumors in the dataset, {:.2f}% of them were classified as malignant by the model.".format(recall*100))

# Precision: Accuracy of the model's positive predictions --> Out of the positive predictions, how many were actually positive?
## num true positive / (num true positive) + (num false positive)
precision = precision_score(Yva, y_pred)
print("\n Precision Score on test data, using best random forest model with optimal hyperparameters: {:.2f}".format(precision))
print("Out of the tumors that the model classified as malignant, {:.2f}% of them were actually malignant.".format(precision*100))

# Accuracy: Proportion of Correctly Predicted Cases (Class 0 & Class 1) = (num true positive + num true negative) / total number of predictions
accuracy = best_rf.score(Xva, Yva)
print("\n Accuracy on test data, using best random forest model with optimal hyperparameters: {:.2f}".format(accuracy))
print("Out of all the tumors in the dataset, {:.2f}% of them were correctly classified by the model.".format(accuracy*100))

"""#### Cross Validation

Even though we already used GridSearchCV to determine the best hyperparameter choices for the random forest model, this does not ensure that our model will generalize well to unseen data. 

We should still use cross validation to estimate the performance of our model on unseen data. We will still use stratified k-fold cross validation to also account for the imbalanced data.

In the context of medical diagnosis, it is important to prioritize high precision and high recall. F1 score is a model performance measure that captures both precision and recall, and so we want this number to be as close to 1 as possible, which would represent perfect precision and recall.
"""

accuracies = []
precisions = []
recalls = []
f1_scores = []

skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

starttime= time.process_time()

for fold_i, (train_index, test_index) in enumerate(skf.split(X, Y)):
    X_train, X_test = X.loc[train_index], X.loc[test_index]
    y_train, y_test = Y[train_index], Y[test_index]

    # Already created instance of random forest classifer with best hyperparameters, just need to fit 
    best_rf.fit(X_train, y_train)
    y_pred = best_rf.predict(X_test)

    # Performance measures after each iteration of K-fold cross validation
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1_score = (2 * (precision * recall) / (precision + recall))

    accuracies.append(accuracy)
    precisions.append(precision)
    recalls.append(recall)
    f1_scores.append(f1_score)

    print("FOLD: ", fold_i+1)
    print("Precision: {:.2f}".format(precision))
    print("Recall: {:.2f}".format(recall))
    print("F1 Score: {:.2f}".format(f1_score))
    print("Accuracy: {:.2f}".format(accuracy))
    print("\n")

endtime= time.process_time()

totaltime= endtime- starttime
print(f"Total time taken by 10-fold Cross Validation with Random Forest Classifier is" ,(totaltime),'seconds')

print("Average Peformance Measures of Random Forest Model Using {}-fold Cross Validation".format(num_folds))
print("Precision: ", np.mean(precisions))
print("Recall: ", np.mean(recalls))
print("F1 Score: ", np.mean(f1_scores))
print("Accuracy: ", np.mean(accuracies))

"""These average performance measures are estimates of the model's performance on new, unseen data. The random forest model with hyperparameter choices generated from GridSearchCV is expected to perform well on unseen data, with high precision and high recall, as indicated by the F1 score of 0.91.

## Gradient Boost: Adaptive Boosting (AdaBoost)

AdaBoost is a gradient boosting algorithm that, similar to random forest, averages over many weak base learners to produce a strong classifier. The base learners are produced sequentially, for the reason being that each base learner focuses on the previous learner's misclassified samples. The misclassifed samples of one base learner are assigned higher weights, making them more likely to be selected multiple times to be apart of the next dataset belonging to the next base learner. Thus, the next dataset will contain more incorrectly classified examples, causing the next base learner to improve on their classification. This process is done iteratively on the base learners to improve the classification accuracy of the model. 

Advantage:

The primary reason we chose AdaBoost is because it handles imbalanced data (to some degree) by assigning higher weights to misclassified samples (the minority class) in order to improve overall accuracy.

Disadvantage:

When the dataset contains highly correlated features, the weak learners in AdaBoost may place too much emphasis on those features, leading to overfitting and reduced generalization ability. In particular, if two or more features are highly correlated, they may provide similar information to the model, which can lead to a situation where AdaBoost ends up selecting one feature over the other in each iteration, without improving the overall performance of the model.

Ways to Improve:

- Increase the number of estimators: AdaBoost works by combining multiple weak learners, or estimators, into a strong ensemble. By increasing the number of estimators, you can improve the model's accuracy and reduce overfitting. However, adding too many estimators can lead to longer training times and may not necessarily improve the performance.

- Adjust the learning rate: AdaBoost updates the weights of misclassified examples at each iteration, which determines the contribution of each estimator to the final prediction. The learning rate controls the amount of weight update at each iteration, and adjusting it can help to reduce overfitting and improve the model's performance.

more misclassifications > lesser amt of say stump has in final ensemble > higher weights for misclassified samples in next dataset > more focus next stump will place on misclassified samples > if next decision stump misclassifies the misclassified samples again, it will cause their weights to increase once again (and so on and so forth) > overfitting to misclassified samples ???

----------------------------------------------------------------------------

- Regularization: Adding regularization terms, such as L1 or L2 regularization, to the AdaBoost algorithm can help to prevent overfitting and improve the model's generalization ability. Regularization adds a penalty term to the objective function that discourages the model from fitting the noise in the data.

- Feature engineering and selection: Feature engineering involves transforming the raw features of the dataset into a form that is more suitable for the model, such as scaling, encoding, or generating new features. Feature selection involves selecting a subset of the most informative features that can improve the model's accuracy and reduce overfitting.

----------------------------------------------------------------------------

- Use more complex base models: AdaBoost works by combining a sequence of weak classifiers, which can be any type of model that performs better than random guessing. If the base model is too simple, it may not be able to capture the underlying patterns in the data. Therefore, using more complex models as base estimators, such as decision trees with more depth, can lead to better performance.

- Ensemble methods: Combining multiple AdaBoost models with different hyperparameters or base estimators using ensemble methods such as stacking or blending can further improve the performance and robustness of the model.

#### Removing Highly Correlated Features

First, it's generally a good idea to remove highly correlated features before using AdaBoost. Highly correlated features in a model can slow down the training process, especially for computationally intensive algorithms like AdaBoost. We construct a correlation matrix and identify pairs of features with a correlation higher than 0.70, and remove one of the features from each pair.
"""

mean_features = data.iloc[:, 1:12]

X = mean_features.drop(['diagnosis'], axis=1)
Y = mean_features['diagnosis']

plt.figure(figsize=(12, 9))
plt.title("Correlation Graph (mean features only)")
cols = list(X.columns)
sns.heatmap(data[cols].corr(), annot=True);

corr_matrix = X.corr().abs()

upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]

# Drop the highly correlated features
plt.title("Correlation Graph: Pairs with <0.7 Correlation")
feats = X.drop(to_drop, axis=1)
sns.heatmap(data[list(feats.columns)].corr(), annot=True);

"""#### AdaBoost Classifier

- `n_estimators` is the number of base learners (the base learner is a decision tree with 1 feature, known as a decision stump). 

- `learning_rate` is a factor that scales the contribution of each base learner before adding it the final ensemble. A small learning rate means that the classifier will take smaller steps towards the optimal solution and may require more iterations converge, however it may result in higher accuracy and lower variance. A large learning rate means that the classifier will take bigger steps towards the optimal solution and may require fewer iterations to converge, however it may result in lower accuracy and higher variance. It is necessary to tune this hyperparameter to prevent overfitting in the case that the weighted contribution of each base learner is too large.

#### Grid Search CV
"""

from sklearn.ensemble import AdaBoostClassifier 
from sklearn.model_selection import GridSearchCV

ada = AdaBoostClassifier()

# Define the hyperparameter grid
param_grid = {'n_estimators': np.arange(10, 210, 10),
              'learning_rate': [0.01, 0.1, 1, 1.5, 2.0]}

# Perform grid search with 5-fold cross-validation
grid_search = GridSearchCV(ada, param_grid, cv=5)
grid_search.fit(feats, Y)

# Print the best hyperparameters
print("Best parameters:", grid_search.best_params_)

"""#### Validation Curve

sklearn `validation_curve` helps to visualize the effect of varying model hyperparameters on the performance of the model by plotting the model's performance on the training and validation sets as a function of the hyperparameter's value. Generates a plot to show model's performance on traaining and validation data, as a function of the hyperparameter values. Hyperparameters can be any parameter that affects training and model performance is measured via performance metrics such as accuracy.

We will look at how training accuracy and validation accuracy change as `n_estimators` and `learning_rate` varies.
"""

from sklearn.model_selection import validation_curve
from sklearn.ensemble import AdaBoostClassifier
import numpy as np

# plot validation curve to visualize effect of learning rate on model performance
train_scores, test_scores = validation_curve(ada, feats, Y, param_name='n_estimators', param_range=np.arange(10, 210, 10), cv=5, scoring="accuracy")

plt.figure()
plt.plot(np.arange(10, 210, 10), np.mean(train_scores, axis=1), label='Training score')
plt.plot(np.arange(10, 210, 10), np.mean(test_scores, axis=1), label='Cross-validation score')
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.title('Validation Curve')
plt.legend(loc='best')
plt.show()

# plot validation curve to visualize effect of learning rate on model performance
train_scores, test_scores = validation_curve(ada, feats, Y, param_name='learning_rate', param_range=[0.01, 0.1, 1, 1.5, 2.0], cv=5, scoring="accuracy")

plt.figure()
plt.plot([0.01, 0.1, 1, 1.5, 2.0], np.mean(train_scores, axis=1), label='Training score')
plt.plot([0.01, 0.1, 1, 1.5, 2.0], np.mean(test_scores, axis=1), label='Cross-validation score')
plt.xlabel('Learning Rate')
plt.ylabel('Accuracy')
plt.title('Validation Curve')
plt.legend(loc='best')
plt.show()

"""Training and cross-validationn accuracy both drop significantly once the learning rate exceeds 1.50. The peak of training and cross-validation accuracy is at a learning rate of 1.50.

#### Train AdaBoost Classifier with Optimal Values for `n_estimators` and `learning_rate`
"""

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

# Create an instance of AdaBoostClassifier
ada_boost = AdaBoostClassifier(n_estimators=100, learning_rate = 1.5, random_state=0)

# Perform cross-validation and compute the metrics
scoring = ['precision_macro', 'recall_macro', 'f1_macro', 'accuracy']
starttime= time.process_time()
scores = cross_validate(ada_boost, feats, Y, scoring=scoring, cv=10)
endtime= time.process_time()

totaltime = endtime-starttime
print(f"Total time taken by AdaBoost Classifier is" ,(totaltime),'seconds')

# Print the metrics
print("Precision:", np.mean(scores['test_precision_macro']))
print("Recall:", np.mean(scores['test_recall_macro']))
print("F1 score:", np.mean(scores['test_f1_macro']))
print("Accuracy:", np.mean(scores['test_accuracy']))

"""## Further Possible Improvements to AdaBoost

#### Scaling Features

The AdaBoost algorithm itself is based on decision trees, which can handle both continuous and categorical features with varying scales and ranges.

However, scaling features can still be beneficial for AdaBoost in some cases. For example, if the range of a feature is much larger than that of other features, then it could dominate the learning process and make the algorithm biased towards that feature. Scaling the features can help prevent this bias and ensure that each feature is treated equally.

In general, it's a good idea to experiment with both scaled and unscaled features and compare the performance of the AdaBoost classifier in each case. If scaling improves the performance, then it may be worth using. However, if scaling does not have a significant effect on performance, or if it harms performance, then it may be unnecessary to scale the features.
"""

mean_features = data.iloc[:, 1:12]

X = mean_features.drop(['diagnosis'], axis=1)
Y = mean_features['diagnosis']

# Scale mean features only
scaler = StandardScaler()  
X_scaled = scaler.fit_transform(X)

# Create new DF of scaled features
cols = list(X.columns)
X_scaled = pd.DataFrame(X_scaled, columns=cols)

# Correlation Matrix of scaled features
plt.figure(figsize=(12, 9))
plt.title("Correlation Graph (scaled mean features only)")
sns.heatmap(X_scaled.corr(), annot=True)

corr_matrix = X_scaled.corr().abs()

upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]

# Drop the highly correlated features
plt.title("Correlation Graph: Pairs with <0.7 Correlation")
X_scaled = X_scaled.drop(to_drop, axis=1)
sns.heatmap(data[list(X_scaled.columns)].corr(), annot=True);

"""#### Lasso Cross-Validation (Feature Selection)

Although scaling features did not affect correlations between features and therefore did not change which highly correlated features should be removed, we will still use the scaled features for Lasso Cross-Validation. 

Lasso Cross-Validation works by shrinking the coefficients of irrelevant features to zero, and keeping only the most important features - thereby reducing complexity and simplifying the model. It penalizes features with large magnitudes, so it is important that all features to be considered all share the same scale. Scaling the features ensures that they are all penalized equally based on their predictive power, and not based on their scale.
"""

from sklearn.linear_model import LassoCV

reg = LassoCV()
reg.fit(X_scaled, Y)


print("Best alpha using built-in LassoCV: %f" % reg.alpha_)
print("Best score using built-in LassoCV: %f" %reg.score(X_scaled, Y))
coef = pd.Series(reg.coef_, index = X_scaled.columns)

print("Lasso picked " + str(sum(coef != 0)) + " variables and eliminated the other " +  
      str(sum(coef == 0)) + " variables")

imp_coef = coef.abs().sort_values()
import matplotlib
matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)
imp_coef.plot(kind = "barh", color= 'pink')
plt.title("Feature importance using Lasso Model")
plt.show()

"""Lasso CV did not improve the performance of our model. It did not remove any features. A possible reason is because there are too few remaining features or too many highly correlated features. However, we know that because we removed highly correlated features prior to using Lasso CV, this is not the reason why. The more plausible reason is because there are too few features, so let's start out with all the original features (bigger set of features, instead of just the `mean_features`), scale them, remove the highly correlated features, conduct Lasso CV, then estimate the model's performance."""

all_feats = data.iloc[:, 2:]

X = all_feats
Y = data.loc[:, 'diagnosis']

# Scale *all* features
scaler = StandardScaler()  
X_scaled = scaler.fit_transform(X)

# Create new DF with scaled features
cols = list(X.columns)
X_scaled = pd.DataFrame(X_scaled, columns=cols)

# Correlation matrix of scaled features
corr_matrix = X_scaled.corr().abs()

# Drop the highly correlated features
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]
X_scaled = X_scaled.drop(to_drop, axis=1)

# Conduct Lasso CV
reg = LassoCV()
reg.fit(X_scaled, Y)


print("Best alpha using built-in LassoCV: %f" % reg.alpha_)
print("Best score using built-in LassoCV: %f" %reg.score(X_scaled, Y))
coef = pd.Series(reg.coef_, index = X_scaled.columns)

print("Lasso picked " + str(sum(coef != 0)) + " variables and eliminated the other " +  
      str(sum(coef == 0)) + " variables")

imp_coef = coef.abs().sort_values()
matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)
imp_coef.plot(kind = "barh", color= 'pink')
plt.title("Feature importance using Lasso Model")
plt.show()

ada = AdaBoostClassifier()

# Define the hyperparameter grid
param_grid = {'n_estimators': np.arange(10, 210, 10),
              'learning_rate': [0.01, 0.1, 1, 1.5, 2.0]}

# Perform grid search with 5-fold cross-validation
grid_search = GridSearchCV(ada, param_grid, cv=5)
grid_search.fit(X_scaled, Y)

# Print the best hyperparameters
print("Best parameters:", grid_search.best_params_)

from sklearn.model_selection import validation_curve
from sklearn.ensemble import AdaBoostClassifier
import numpy as np

# plot validation curve to visualize effect of learning rate on model performance
train_scores, test_scores = validation_curve(ada, X_scaled, Y, param_name='n_estimators', param_range=np.arange(10, 210, 10), cv=5, scoring="accuracy")

plt.figure()
plt.plot(np.arange(10, 210, 10), np.mean(train_scores, axis=1), label='Training score')
plt.plot(np.arange(10, 210, 10), np.mean(test_scores, axis=1), label='Cross-validation score')
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.title('Validation Curve')
plt.legend(loc='best')
plt.show()

# plot validation curve to visualize effect of learning rate on model performance
train_scores, test_scores = validation_curve(ada, X_scaled, Y, param_name='learning_rate', param_range=[0.01, 0.1, 1, 1.5, 2.0], cv=5, scoring="accuracy")

plt.figure()
plt.plot([0.01, 0.1, 1, 1.5, 2.0], np.mean(train_scores, axis=1), label='Training score')
plt.plot([0.01, 0.1, 1, 1.5, 2.0], np.mean(test_scores, axis=1), label='Cross-validation score')
plt.xlabel('Learning Rate')
plt.ylabel('Accuracy')
plt.title('Validation Curve')
plt.legend(loc='best')
plt.show()

# Create an instance of AdaBoostClassifier
ada_boost = AdaBoostClassifier(n_estimators=80, learning_rate = 1.5, random_state=0)

# Perform cross-validation and compute the metrics
scoring = ['precision_macro', 'recall_macro', 'f1_macro', 'accuracy']

starttime= time.process_time()
scores = cross_validate(ada_boost, X_scaled, Y, scoring=scoring, cv=10)
endtime= time.process_time()

totaltime = endtime-starttime
print(f"Total time taken by AdaBoost Classifier is" ,(totaltime),'seconds')

# Print the metrics
print("Precision:", np.mean(scores['test_precision_macro']))
print("Recall:", np.mean(scores['test_recall_macro']))
print("F1 score:", np.mean(scores['test_f1_macro']))
print("Accuracy:", np.mean(scores['test_accuracy']))

"""Even after considering all 30 features instead of just the 10 mean_features, our model performance improved by a little. Although there were 4 additional features (10 features total) in the more complex model, the training time didn't take much longer compared to when there  were 6 features. Precision, recall, F1 score, and accuracy all increased with little additional training time.

Let's try to improve this model by varying the depth of base learners.
"""

param_grid = {'base_estimator__max_depth': [1, 2, 3, 4, 5]} 
base_estimator = DecisionTreeClassifier()
adaboost = AdaBoostClassifier(base_estimator=base_estimator, learning_rate=1.5, n_estimators=80)
grid_search = GridSearchCV(adaboost, param_grid=param_grid, cv=5)
grid_search.fit(X_scaled, y)
print("Best parameters:", grid_search.best_params_)
print("Best score:", grid_search.best_score_)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import validation_curve

base_estimator = DecisionTreeClassifier()
adaboost = AdaBoostClassifier(base_estimator=base_estimator, learning_rate=1.5, n_estimators=80)

param_range = np.arange(1, 6)
train_scores, test_scores = validation_curve(adaboost, X_scaled, Y, param_name='base_estimator__max_depth', 
                                             param_range=param_range, cv=5, scoring='accuracy')

train_scores_mean = np.mean(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)

plt.plot(param_range, train_scores_mean, label='Training score')
plt.plot(param_range, test_scores_mean, label='Cross-validation score')
plt.xlabel('Max depth of the decision tree')
plt.ylabel('Accuracy')
plt.title('Validation curve')
plt.legend()
plt.show()